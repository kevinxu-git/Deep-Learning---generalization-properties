# Projects on ML Generalization Properties
> February-March 2020 : Master 2 Data Science at Ecole Polytechnique in France

+ Projects for the course Generalisation properties of algorithms in ML

# Project 1 : Batch Normalization and Dropout study
Batch Normalization and Dropout are standard techniques for the regularisation of deep networks.
The aim of this project is to explain intuitively how these two techniques work, what problem they aim to solve and how to use them effectively. To this end, this presentation has been structured as follows :
1. Mechanism of Batch Normalization and its Effects
2. Mechanism of Dropout and its Effects
3. Illustrative Experiments

We provided : 
+ A Jupyter Notebook with the experiments carried on CIFAR-10 image classification
+ A presentation 
+ A detailed report
 
# Project 2 : Presentation of a paper
The paper : 
> [Generalization Properties and Implicit Regularization for Multiple Passes SGM, Lin J., Camoriano R., Rosasco L.](http://proceedings.mlr.press/v48/lina16.pdf)

The aim of the paper is to analyze how the step-size (a.k.a learning rate) and the number of passes in the stochastic gradient method (SGM) induce an implicit regularization of the model. This is done by :
+ Finding explicit bounds on the generalization risk that depend on the step-size and the number of passes.
+ Exploiting different strategies for setting the step-sizes and number of passes to optimize this bound, thus showing the regularisation effect of these parameters.

Our goal was to clearly and easily explain the regularization effect of these parameters to people that have not read the paper.

# Authors
+ **Kevin XU** - *Master 2 Data Science student* - [kevinxu-git](https://github.com/kevinxu-git)
+ **Pierre ADEIKALAM** - *Master 2 Data Science student*
+ **Guangyue CHEN** - *Master 2 Data Science student* 
